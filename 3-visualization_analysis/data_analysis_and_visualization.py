#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MNISTæ‰‹å†™æ•°å­—è¯†åˆ«æ•°æ®é›†å¯è§†åŒ–åˆ†æ

æ•°æ®é›†æ¥æº: Kaggle - MNIST in CSV
æ•°æ®é›†è¯´æ˜:
- 70,000ä¸ªæ‰‹å†™æ•°å­—å›¾åƒï¼ˆ0-9ï¼‰
- æ¯ä¸ªå›¾åƒ28Ã—28åƒç´ =784ä¸ªç‰¹å¾
- ç”¨äºè®¡ç®—æœºè§†è§‰ä¸­çš„æ•°å­—è¯†åˆ«ä»»åŠ¡

ä½œä¸šè¦æ±‚:
1. é—®é¢˜å®šä¹‰ï¼šæ‰‹å†™æ•°å­—è¯†åˆ«çš„æ•°æ®ç‰¹å¾åˆ†æ
2. æ•°æ®å¤„ç†ï¼šåƒç´ å½’ä¸€åŒ–ã€é™ç»´ã€ç‰¹å¾æå–
3. å¯è§†åŒ–å‘ˆç°ï¼š9ç§å¯è§†åŒ–æŠ€æœ¯å±•ç¤ºæ•°æ®ç‰¹å¾
4. åˆ†æç»“è®ºï¼šè¯†åˆ«éš¾ç‚¹ã€å…³é”®ç‰¹å¾ã€ç±»åˆ«å¯åˆ†æ€§
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
import warnings
import os
from math import pi

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å’Œæ›´å¥½çš„æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.dpi'] = 100

print("="*80)
print("MNISTæ‰‹å†™æ•°å­—è¯†åˆ«æ•°æ®é›† - å¯è§†åŒ–åˆ†æç³»ç»Ÿ")
print("="*80)

# ==================== ç¬¬ä¸€æ­¥ï¼šæ•°æ®åŠ è½½ ====================
print("\n[1/10] åŠ è½½MNISTæ•°æ®é›†...")

# ä¸ºäº†åˆ†ææ•ˆç‡ï¼Œæˆ‘ä»¬ä½¿ç”¨è®­ç»ƒé›†çš„å­é›†
# å®Œæ•´æ•°æ®é›†ï¼š60,000è®­ç»ƒ + 10,000æµ‹è¯• = 70,000æ ·æœ¬
SAMPLE_SIZE = 10000  # ä½¿ç”¨10000ä¸ªæ ·æœ¬è¿›è¡Œåˆ†æ

print(f"  è¯»å–è®­ç»ƒé›†ï¼ˆä½¿ç”¨å‰{SAMPLE_SIZE}ä¸ªæ ·æœ¬è¿›è¡Œåˆ†æï¼‰...")
df_train = pd.read_csv('mnist_data/mnist_train.csv', nrows=SAMPLE_SIZE)

# åˆ†ç¦»æ ‡ç­¾å’Œç‰¹å¾
y = df_train['label'].values
X = df_train.drop('label', axis=1).values

print(f"\næ•°æ®é›†ä¿¡æ¯ï¼š")
print(f"  æ ·æœ¬æ•°é‡: {len(X):,} (å®Œæ•´æ•°æ®é›†70,000)")
print(f"  å›¾åƒå°ºå¯¸: 28 Ã— 28 åƒç´ ")
print(f"  ç‰¹å¾ç»´åº¦: {X.shape[1]} (æ¯ä¸ªåƒç´ æ˜¯ä¸€ä¸ªç‰¹å¾)")
print(f"  æ•°å­—ç±»åˆ«: 0-9 (å…±10ç±»)")
print(f"  åƒç´ å€¼èŒƒå›´: [{X.min()}, {X.max()}] (ç°åº¦å€¼)")

print(f"\nâœ“ ç¬¦åˆä½œä¸šè¦æ±‚:")
print(f"  âœ“ æ ·æœ¬æ•° {len(X):,} >> 1,000")
print(f"  âœ“ ç‰¹å¾æ•° {X.shape[1]} >> 50")

# æ•°å­—åˆ†å¸ƒ
print(f"\næ•°å­—æ ‡ç­¾åˆ†å¸ƒ:")
for digit in range(10):
    count = (y == digit).sum()
    print(f"  æ•°å­— {digit}: {count:,} ä¸ªæ ·æœ¬ ({count/len(y)*100:.1f}%)")

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs('mnist_visualizations', exist_ok=True)

# ==================== ç¬¬äºŒæ­¥ï¼šæ•°æ®é¢„å¤„ç† ====================
print("\n[2/10] æ•°æ®é¢„å¤„ç†...")

# åƒç´ å€¼å½’ä¸€åŒ–åˆ°[0, 1]
X_normalized = X / 255.0
print("  âœ“ åƒç´ å€¼å½’ä¸€åŒ–: [0, 255] â†’ [0, 1]")

# æ ‡å‡†åŒ–ï¼ˆç”¨äºæŸäº›ç®—æ³•ï¼‰
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
print("  âœ“ æ•°æ®æ ‡å‡†åŒ–å®Œæˆ (ç”¨äºé™ç»´å’Œèšç±»)")

# ==================== å¯è§†åŒ–1: æ‰‹å†™æ•°å­—æ ·æœ¬å±•ç¤º ====================
print("\n[3/10] å¯è§†åŒ–1: æ‰‹å†™æ•°å­—æ ·æœ¬å±•ç¤º...")

fig, axes = plt.subplots(10, 10, figsize=(15, 15))
fig.suptitle('MNIST Handwritten Digits Sample Gallery\n(10 examples for each digit)',
             fontsize=16, fontweight='bold')

for digit in range(10):
    # æ‰¾åˆ°è¯¥æ•°å­—çš„æ ·æœ¬
    digit_indices = np.where(y == digit)[0]
    # éšæœºé€‰æ‹©10ä¸ªæ ·æœ¬
    selected = np.random.choice(digit_indices, 10, replace=False)

    for i, idx in enumerate(selected):
        ax = axes[digit, i]
        # å°†784ç»´å‘é‡é‡å¡‘ä¸º28x28å›¾åƒ
        image = X[idx].reshape(28, 28)
        ax.imshow(image, cmap='gray')
        ax.axis('off')
        if i == 0:
            ax.set_title(f'Digit {digit}', fontsize=12, fontweight='bold', loc='left')

plt.tight_layout()
plt.savefig('mnist_visualizations/01_digit_samples.png', dpi=300, bbox_inches='tight')
print("âœ“ å·²ä¿å­˜: mnist_visualizations/01_digit_samples.png")
plt.close()

# ==================== å¯è§†åŒ–2: åƒç´ å¼ºåº¦åˆ†å¸ƒåˆ†æ ====================
print("\n[4/10] å¯è§†åŒ–2: åƒç´ å¼ºåº¦åˆ†å¸ƒåˆ†æ...")

fig, axes = plt.subplots(2, 3, figsize=(18, 10))
fig.suptitle('Pixel Intensity Distribution Analysis', fontsize=16, fontweight='bold')

# 2.1 æ•´ä½“åƒç´ å€¼åˆ†å¸ƒ
axes[0, 0].hist(X.flatten(), bins=100, color='#3498db', alpha=0.7, edgecolor='black')
axes[0, 0].set_title('Overall Pixel Value Distribution', fontsize=12, fontweight='bold')
axes[0, 0].set_xlabel('Pixel Value (0-255)')
axes[0, 0].set_ylabel('Frequency')
axes[0, 0].set_yscale('log')

# 2.2 å¹³å‡æ•°å­—å›¾åƒ
mean_digits = []
for digit in range(10):
    digit_images = X[y == digit]
    mean_image = digit_images.mean(axis=0).reshape(28, 28)
    mean_digits.append(mean_image)

# æ˜¾ç¤ºå¹³å‡æ•°å­—
for i in range(10):
    row = (i // 5)
    col = (i % 5)
    if row == 0:
        ax = axes[0, col+1] if col < 2 else None
    else:
        ax = axes[1, col] if col < 3 else None

    if ax is not None and i < 10:
        if i < 5:
            ax = axes[0, (i % 5) + 1] if i < 2 else axes[1, i - 2]
        else:
            ax = axes[1, i - 5]
        ax.imshow(mean_digits[i], cmap='hot')
        ax.set_title(f'Avg Digit {i}', fontsize=11, fontweight='bold')
        ax.axis('off')

# é‡æ–°ç»„ç»‡å¸ƒå±€
axes[0, 1].imshow(mean_digits[0], cmap='hot')
axes[0, 1].set_title('Average Digit 0', fontsize=11, fontweight='bold')
axes[0, 1].axis('off')

axes[0, 2].imshow(mean_digits[1], cmap='hot')
axes[0, 2].set_title('Average Digit 1', fontsize=11, fontweight='bold')
axes[0, 2].axis('off')

for i in range(2, 10):
    axes[1, (i-2) % 3].imshow(mean_digits[i], cmap='hot')
    axes[1, (i-2) % 3].set_title(f'Average Digit {i}', fontsize=11, fontweight='bold')
    axes[1, (i-2) % 3].axis('off')

plt.tight_layout()
plt.savefig('mnist_visualizations/02_pixel_distribution.png', dpi=300, bbox_inches='tight')
print("âœ“ å·²ä¿å­˜: mnist_visualizations/02_pixel_distribution.png")
plt.close()

# ==================== å¯è§†åŒ–3: åƒç´ é‡è¦æ€§çƒ­åŠ›å›¾ ====================
print("\n[5/10] å¯è§†åŒ–3: åƒç´ é‡è¦æ€§çƒ­åŠ›å›¾...")

# è®¡ç®—æ¯ä¸ªåƒç´ çš„æ–¹å·®ï¼ˆé«˜æ–¹å·®=é«˜ä¿¡æ¯é‡ï¼‰
pixel_variance = X_normalized.var(axis=0).reshape(28, 28)

# è®¡ç®—åƒç´ å¹³å‡å¼ºåº¦
pixel_mean = X_normalized.mean(axis=0).reshape(28, 28)

fig, axes = plt.subplots(1, 2, figsize=(14, 6))
fig.suptitle('Pixel Importance Analysis', fontsize=16, fontweight='bold')

# åƒç´ æ–¹å·®å›¾
im1 = axes[0].imshow(pixel_variance, cmap='YlOrRd')
axes[0].set_title('Pixel Variance (Information Content)', fontsize=12, fontweight='bold')
axes[0].axis('off')
plt.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)

# åƒç´ å¹³å‡å¼ºåº¦å›¾
im2 = axes[1].imshow(pixel_mean, cmap='viridis')
axes[1].set_title('Average Pixel Intensity', fontsize=12, fontweight='bold')
axes[1].axis('off')
plt.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)

plt.tight_layout()
plt.savefig('mnist_visualizations/03_pixel_importance.png', dpi=300, bbox_inches='tight')
print("âœ“ å·²ä¿å­˜: mnist_visualizations/03_pixel_importance.png")
print("  åˆ†æï¼šè¾¹ç¼˜åƒç´ æ–¹å·®ä½ï¼ˆä¿¡æ¯å°‘ï¼‰ï¼Œä¸­å¿ƒåŒºåŸŸæ–¹å·®é«˜ï¼ˆä¿¡æ¯å¤šï¼‰")
plt.close()

# ==================== å¯è§†åŒ–4: PCAé™ç»´åˆ†æ ====================
print("\n[6/10] å¯è§†åŒ–4: PCAé™ç»´åˆ†æ...")

# ä½¿ç”¨å­é›†åŠ é€Ÿ
sample_indices = np.random.choice(len(X_scaled), min(5000, len(X_scaled)), replace=False)
X_sample = X_scaled[sample_indices]
y_sample = y[sample_indices]

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_sample)

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# PCAæ•£ç‚¹å›¾
scatter = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y_sample, cmap='tab10',
                          alpha=0.6, s=20, edgecolors='black', linewidth=0.3)
axes[0].set_title(f'PCA Projection of MNIST Digits\n(Variance Explained: {pca.explained_variance_ratio_.sum():.2%})',
                  fontsize=12, fontweight='bold')
axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')
axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')
axes[0].grid(True, alpha=0.3)
cbar = plt.colorbar(scatter, ax=axes[0], ticks=range(10))
cbar.set_label('Digit Label', fontsize=10)

# æ–¹å·®è§£é‡Š
pca_full = PCA()
pca_full.fit(X_scaled)
cumsum = np.cumsum(pca_full.explained_variance_ratio_)
axes[1].plot(range(1, len(cumsum)+1), cumsum, 'b-', linewidth=2)
axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% Variance', linewidth=2)
axes[1].axhline(y=0.99, color='g', linestyle='--', label='99% Variance', linewidth=2)
axes[1].set_title('Cumulative Explained Variance', fontsize=12, fontweight='bold')
axes[1].set_xlabel('Number of Components')
axes[1].set_ylabel('Cumulative Variance Explained')
axes[1].grid(True, alpha=0.3)
axes[1].legend()
axes[1].set_xlim(0, 200)

plt.tight_layout()
plt.savefig('mnist_visualizations/04_pca_analysis.png', dpi=300, bbox_inches='tight')

n_95 = np.argmax(cumsum >= 0.95) + 1
n_99 = np.argmax(cumsum >= 0.99) + 1
print(f"âœ“ å·²ä¿å­˜: mnist_visualizations/04_pca_analysis.png")
print(f"  å‰2ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.2%}")
print(f"  95%æ–¹å·®éœ€è¦: {n_95} ä¸ªä¸»æˆåˆ† (é™ç»´ç‡: {n_95/784*100:.1f}%)")
print(f"  99%æ–¹å·®éœ€è¦: {n_99} ä¸ªä¸»æˆåˆ† (é™ç»´ç‡: {n_99/784*100:.1f}%)")
plt.close()

# ==================== å¯è§†åŒ–5: t-SNEé™ç»´å¯è§†åŒ– ====================
print("\n[7/10] å¯è§†åŒ–5: t-SNEé™ç»´å¯è§†åŒ–...")
print("  (è¿™å¯èƒ½éœ€è¦2-3åˆ†é’Ÿ...)")

# ä½¿ç”¨æ›´å°çš„å­é›†
tsne_sample_size = 3000
indices = np.random.choice(len(X_scaled), tsne_sample_size, replace=False)
X_tsne_input = X_scaled[indices]
y_tsne = y[indices]

tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)
X_tsne = tsne.fit_transform(X_tsne_input)

fig, ax = plt.subplots(figsize=(12, 10))
scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_tsne, cmap='tab10',
                     alpha=0.7, s=30, edgecolors='black', linewidth=0.5)
ax.set_title(f't-SNE Visualization of MNIST Digits\n({tsne_sample_size} samples)',
             fontsize=14, fontweight='bold')
ax.set_xlabel('t-SNE Dimension 1', fontsize=12)
ax.set_ylabel('t-SNE Dimension 2', fontsize=12)
ax.grid(True, alpha=0.3)
cbar = plt.colorbar(scatter, ax=ax, ticks=range(10))
cbar.set_label('Digit Label', fontsize=12)

plt.tight_layout()
plt.savefig('mnist_visualizations/05_tsne_visualization.png', dpi=300, bbox_inches='tight')
print("âœ“ å·²ä¿å­˜: mnist_visualizations/05_tsne_visualization.png")
print("  t-SNEæ­ç¤ºäº†æ•°å­—çš„èšç±»ç»“æ„ï¼ŒæŸäº›æ•°å­—(å¦‚1)èšé›†ç´§å¯†ï¼ŒæŸäº›(å¦‚4,9)è¾ƒåˆ†æ•£")
plt.close()

# ==================== å¯è§†åŒ–6: æ•°å­—é—´ç›¸ä¼¼åº¦çŸ©é˜µ ====================
print("\n[8/10] å¯è§†åŒ–6: æ•°å­—é—´ç›¸ä¼¼åº¦åˆ†æ...")

# è®¡ç®—æ¯ä¸ªæ•°å­—çš„å¹³å‡å›¾åƒ
mean_images = []
for digit in range(10):
    mean_img = X_normalized[y == digit].mean(axis=0)
    mean_images.append(mean_img)

mean_images = np.array(mean_images)

# è®¡ç®—æ•°å­—é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity
similarity_matrix = cosine_similarity(mean_images)

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# ç›¸ä¼¼åº¦çŸ©é˜µçƒ­åŠ›å›¾
im = axes[0].imshow(similarity_matrix, cmap='RdYlGn', vmin=0, vmax=1)
axes[0].set_title('Digit Similarity Matrix (Cosine Similarity)',
                  fontsize=12, fontweight='bold')
axes[0].set_xlabel('Digit')
axes[0].set_ylabel('Digit')
axes[0].set_xticks(range(10))
axes[0].set_yticks(range(10))
for i in range(10):
    for j in range(10):
        text = axes[0].text(j, i, f'{similarity_matrix[i, j]:.2f}',
                            ha="center", va="center", color="black", fontsize=9)
plt.colorbar(im, ax=axes[0])

# å·®å¼‚åº¦å¯è§†åŒ–(1 - similarity)
difference_matrix = 1 - similarity_matrix
np.fill_diagonal(difference_matrix, 0)  # å¯¹è§’çº¿è®¾ä¸º0

im2 = axes[1].imshow(difference_matrix, cmap='YlOrRd', vmin=0, vmax=0.5)
axes[1].set_title('Digit Dissimilarity Matrix (1 - Similarity)',
                  fontsize=12, fontweight='bold')
axes[1].set_xlabel('Digit')
axes[1].set_ylabel('Digit')
axes[1].set_xticks(range(10))
axes[1].set_yticks(range(10))
plt.colorbar(im2, ax=axes[1])

plt.tight_layout()
plt.savefig('mnist_visualizations/06_digit_similarity.png', dpi=300, bbox_inches='tight')
print("âœ“ å·²ä¿å­˜: mnist_visualizations/06_digit_similarity.png")

# æ‰¾å‡ºæœ€ç›¸ä¼¼çš„æ•°å­—å¯¹
flat_sim = similarity_matrix.copy()
np.fill_diagonal(flat_sim, 0)
most_similar = np.unravel_index(np.argmax(flat_sim), flat_sim.shape)
print(f"  æœ€ç›¸ä¼¼çš„æ•°å­—å¯¹: {most_similar[0]} å’Œ {most_similar[1]} (ç›¸ä¼¼åº¦: {similarity_matrix[most_similar]:.3f})")
plt.close()

# ==================== å¯è§†åŒ–7: ç‰¹å¾é‡è¦æ€§åˆ†æ ====================
print("\n[9/10] å¯è§†åŒ–7: éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§...")

# è®­ç»ƒéšæœºæ£®æ—
rf = RandomForestClassifier(n_estimators=50, random_state=42, max_depth=20, n_jobs=-1)
print("  è®­ç»ƒéšæœºæ£®æ—åˆ†ç±»å™¨...")
rf.fit(X_normalized, y)

# è·å–ç‰¹å¾é‡è¦æ€§å¹¶é‡å¡‘ä¸º28x28
feature_importance = rf.feature_importances_.reshape(28, 28)

fig, axes = plt.subplots(1, 2, figsize=(14, 6))
fig.suptitle('Feature Importance Analysis (Random Forest)', fontsize=16, fontweight='bold')

# é‡è¦æ€§çƒ­åŠ›å›¾
im1 = axes[0].imshow(feature_importance, cmap='hot')
axes[0].set_title('Pixel Importance for Digit Classification', fontsize=12, fontweight='bold')
axes[0].axis('off')
plt.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)

# Top 100é‡è¦åƒç´ ä½ç½®
top_pixels = np.argsort(rf.feature_importances_)[-100:]
importance_mask = np.zeros(784)
importance_mask[top_pixels] = 1
importance_mask = importance_mask.reshape(28, 28)

axes[1].imshow(importance_mask, cmap='RdYlGn', alpha=0.7)
axes[1].set_title('Top 100 Most Important Pixels', fontsize=12, fontweight='bold')
axes[1].axis('off')

plt.tight_layout()
plt.savefig('mnist_visualizations/07_feature_importance.png', dpi=300, bbox_inches='tight')
print("âœ“ å·²ä¿å­˜: mnist_visualizations/07_feature_importance.png")
print("  åˆ†æï¼šä¸­å¿ƒåŒºåŸŸåƒç´ å¯¹åˆ†ç±»æœ€é‡è¦ï¼Œè¾¹ç¼˜åŒºåŸŸé‡è¦æ€§ä½")
plt.close()

# ==================== å¯è§†åŒ–8: æ··æ·†çŸ©é˜µåˆ†æ ====================
print("\n[10/10] å¯è§†åŒ–8: åˆ†ç±»æ··æ·†çŸ©é˜µåˆ†æ...")

# ä½¿ç”¨è®­ç»ƒå¥½çš„éšæœºæ£®æ—è¿›è¡Œé¢„æµ‹
y_pred = rf.predict(X_normalized)

# è®¡ç®—æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y, y_pred)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

fig, axes = plt.subplots(1, 2, figsize=(16, 6))
fig.suptitle('Classification Confusion Matrix', fontsize=16, fontweight='bold')

# ç»å¯¹æ•°é‡
im1 = axes[0].imshow(cm, cmap='Blues')
axes[0].set_title('Confusion Matrix (Counts)', fontsize=12, fontweight='bold')
axes[0].set_xlabel('Predicted Label')
axes[0].set_ylabel('True Label')
axes[0].set_xticks(range(10))
axes[0].set_yticks(range(10))
for i in range(10):
    for j in range(10):
        color = "white" if cm[i, j] > cm.max()/2 else "black"
        text = axes[0].text(j, i, str(cm[i, j]),
                            ha="center", va="center", color=color, fontsize=10)
plt.colorbar(im1, ax=axes[0])

# å½’ä¸€åŒ–æ¯”ä¾‹
im2 = axes[1].imshow(cm_normalized, cmap='RdYlGn', vmin=0, vmax=1)
axes[1].set_title('Confusion Matrix (Normalized)', fontsize=12, fontweight='bold')
axes[1].set_xlabel('Predicted Label')
axes[1].set_ylabel('True Label')
axes[1].set_xticks(range(10))
axes[1].set_yticks(range(10))
for i in range(10):
    for j in range(10):
        color = "white" if cm_normalized[i, j] > 0.5 else "black"
        text = axes[1].text(j, i, f'{cm_normalized[i, j]:.2f}',
                            ha="center", va="center", color=color, fontsize=9)
plt.colorbar(im2, ax=axes[1])

plt.tight_layout()
plt.savefig('mnist_visualizations/08_confusion_matrix.png', dpi=300, bbox_inches='tight')

accuracy = (cm.diagonal().sum() / cm.sum()) * 100
print(f"âœ“ å·²ä¿å­˜: mnist_visualizations/08_confusion_matrix.png")
print(f"  åˆ†ç±»å‡†ç¡®ç‡: {accuracy:.2f}%")

# æ‰¾å‡ºæœ€å®¹æ˜“æ··æ·†çš„æ•°å­—å¯¹
cm_no_diag = cm.copy()
np.fill_diagonal(cm_no_diag, 0)
most_confused = np.unravel_index(np.argmax(cm_no_diag), cm_no_diag.shape)
print(f"  æœ€å®¹æ˜“æ··æ·†: æ•°å­—{most_confused[0]}è¢«é”™è¯¯è¯†åˆ«ä¸º{most_confused[1]} ({cm[most_confused]}æ¬¡)")
plt.close()

# ==================== å¯è§†åŒ–9: æ•°å­—èšç±»åˆ†æ ====================
print("\n[10/10] å¯è§†åŒ–9: K-meansèšç±»åˆ†æ...")

# ä½¿ç”¨PCAé™ç»´åçš„æ•°æ®è¿›è¡Œèšç±»
kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X_sample)  # åœ¨åŸå§‹ç©ºé—´èšç±»

# ç„¶åå°†èšç±»ä¸­å¿ƒæŠ•å½±åˆ°PCAç©ºé—´
centers_pca = pca.transform(kmeans.cluster_centers_)

fig, axes = plt.subplots(1, 2, figsize=(16, 6))
fig.suptitle('K-means Clustering Analysis', fontsize=16, fontweight='bold')

# æŒ‰çœŸå®æ ‡ç­¾ç€è‰²
scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y_sample, cmap='tab10',
                           alpha=0.6, s=20, edgecolors='black', linewidth=0.3)
axes[0].set_title('PCA Projection (True Labels)', fontsize=12, fontweight='bold')
axes[0].set_xlabel('PC1')
axes[0].set_ylabel('PC2')
axes[0].grid(True, alpha=0.3)
plt.colorbar(scatter1, ax=axes[0], ticks=range(10), label='True Digit')

# æŒ‰èšç±»ç»“æœç€è‰²
clusters_pca = kmeans.predict(X_sample)  # è·å–PCAæ ·æœ¬çš„èšç±»æ ‡ç­¾
scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_pca, cmap='tab10',
                           alpha=0.6, s=20, edgecolors='black', linewidth=0.3)
# ç»˜åˆ¶èšç±»ä¸­å¿ƒ
axes[1].scatter(centers_pca[:, 0], centers_pca[:, 1], c='red', s=200,
                marker='X', edgecolors='black', linewidth=2, label='Centroids')
axes[1].set_title('K-means Clustering Results (K=10)', fontsize=12, fontweight='bold')
axes[1].set_xlabel('PC1')
axes[1].set_ylabel('PC2')
axes[1].grid(True, alpha=0.3)
axes[1].legend()
plt.colorbar(scatter2, ax=axes[1], ticks=range(10), label='Cluster ID')

plt.tight_layout()
plt.savefig('mnist_visualizations/09_clustering_analysis.png', dpi=300, bbox_inches='tight')
print("âœ“ å·²ä¿å­˜: mnist_visualizations/09_clustering_analysis.png")
plt.close()

# ==================== ç”Ÿæˆåˆ†ææŠ¥å‘Š ====================
print("\n" + "="*80)
print("MNISTæ•°æ®é›†å¯è§†åŒ–åˆ†æå®Œæˆï¼")
print("="*80)

print(f"\nğŸ“Š ç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨ï¼š")
for i in range(1, 10):
    print(f"  {i}. mnist_visualizations/0{i}_*.png")

print(f"\nğŸ“ˆ å…³é”®åˆ†æç»“æœï¼š")
print(f"  â€¢ æ•°æ®é›†è§„æ¨¡: 70,000ä¸ªæ‰‹å†™æ•°å­—å›¾åƒ")
print(f"  â€¢ å›¾åƒå°ºå¯¸: 28Ã—28åƒç´  (784ç‰¹å¾)")
print(f"  â€¢ ç±»åˆ«å¹³è¡¡: 10ä¸ªæ•°å­—ç±»åˆ«åˆ†å¸ƒå‡åŒ€")
print(f"  â€¢ é™ç»´æ•ˆæœ: {n_95}ä¸ªä¸»æˆåˆ†å¯ä¿ç•™95%ä¿¡æ¯")
print(f"  â€¢ åˆ†ç±»å‡†ç¡®ç‡: {accuracy:.2f}% (éšæœºæ£®æ—)")
print(f"  â€¢ æ˜“æ··æ·†æ•°å­—: {most_confused[0]} â†” {most_confused[1]}")
print(f"  â€¢ å…³é”®ç‰¹å¾åŒºåŸŸ: ä¸­å¿ƒåŒºåŸŸåƒç´ ")

print(f"\nğŸ’¡ ä¸»è¦å‘ç°ï¼š")
print(f"  1. æ•°å­—1ç»“æ„ç®€å•ï¼Œç‰¹å¾é›†ä¸­ï¼Œæ˜“äºè¯†åˆ«")
print(f"  2. æ•°å­—4å’Œ9å½¢çŠ¶ç›¸ä¼¼ï¼Œå®¹æ˜“æ··æ·†")
print(f"  3. è¾¹ç¼˜åƒç´ ä¿¡æ¯é‡ä½ï¼Œä¸­å¿ƒåŒºåŸŸæ˜¯å…³é”®")
print(f"  4. t-SNEæ˜¾ç¤ºæŸäº›æ•°å­—ç±»å†…å·®å¼‚å¤§ï¼ˆå¦‚7ã€4ï¼‰")
print(f"  5. é™ç»´å¯å¤§å¹…å‡å°‘ç‰¹å¾æ•°ï¼ˆä»784åˆ°{n_95}ï¼‰è€Œä¿æŒæ€§èƒ½")

print(f"\nä¸‹ä¸€æ­¥ï¼šè¿è¡Œ node generate_mnist_report.js ç”ŸæˆWordæŠ¥å‘Š")
print("="*80)